# -*- coding: utf-8 -*-
"""Ejemplo DS-ML Energía Eólica.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e2XcAN4uuuhyP0VwSEy3N270Rv4Ndzrj
"""

# Import the necessary libraries
import numpy as np
import pandas as pd

# Generate a synthetic dataset
np.random.seed(42)
n_samples = 1000
wind_speed = np.random.uniform(3, 25, n_samples)  # Wind speed (m/s)
air_density = np.random.uniform(1.1, 1.3, n_samples)  # Air density (kg/m^3)
blade_length = np.random.uniform(30, 60, n_samples)  # Blade length (m)
efficiency = np.random.uniform(0.3, 0.45, n_samples)  # Turbine efficiency
power_generated = 0.5 * air_density * (np.pi * (blade_length / 2)**2) * wind_speed**3 * efficiency
power_generated += np.random.normal(0, 1e5, n_samples)  # Add noise

# Create a DataFrame
data = pd.DataFrame({
    'wind_speed': wind_speed,
    'air_density': air_density,
    'blade_length': blade_length,
    'efficiency': efficiency,
    'power_generated': power_generated
})

# Save the DataFrame to a CSV file
data.to_csv('wind_energy_data.csv', index=False)

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_squared_log_error
import matplotlib.pyplot as plt

# Load the CSV file (make sure to specify the correct file path)
df = pd.read_csv('wind_energy_data.csv')

# Display the column names
print("Column names:", df.columns)

# Data preprocessing to handle NaN
# Option: Drop rows with NaN
df = df.dropna()


# Initial exploration
print(df.describe())

# Separate features (X) and target variable (y)
X = df.iloc[:, :-1].values  # All columns except the last one
y = df.iloc[:, -1].values   # Only the last column

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred)**0.5
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R-squared (R²): {r2:.2f}")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")

# Cross-validation
cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')
print(f"Cross-validation (MSE): {cv_scores}")
print(f'Average MSE: {-cv_scores.mean()}')
print(f'Standard deviation of MSE: {cv_scores.std()}')

# Hyperparameter tuning with GridSearch
"""
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
print(f"Best GridSearch parameters: {grid_search.best_params_}")
"""

# Feature importance
importances = model.feature_importances_

# Visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(df.columns[:-1], importances, color='skyblue')
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.tight_layout()
plt.show()

# Plot results
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.6, label='Predictions')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', label='Perfect Line')
plt.title('Predictions vs Actual Values')
plt.xlabel('Actual Values')
plt.ylabel('Predictions')
plt.legend()
plt.tight_layout()
plt.show()

# Residual plot
residuals = y_test - y_pred
plt.figure(figsize=(8, 6))
plt.scatter(y_pred, residuals, alpha=0.6)
plt.axhline(0, color='red', linestyle='--')
plt.title('Residuals vs Predictions')
plt.xlabel('Predictions')
plt.ylabel('Residuals')
plt.tight_layout()
plt.show()

# Histogram of residuals
plt.figure(figsize=(8, 6))
plt.hist(residuals, bins=30, color='skyblue', edgecolor='black')
plt.title('Residuals Distribution')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

# Correlation matrix
correlation_matrix = df.corr()
plt.figure(figsize=(10, 8))
plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='none')
plt.colorbar()
plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)
plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)
plt.title('Correlation Matrix')
plt.tight_layout()
plt.show()

# Target variable distribution
plt.figure(figsize=(8, 6))
plt.hist(y, bins=30, color='skyblue', edgecolor='black')
plt.title('Target Variable Distribution')
plt.xlabel('Power Generated')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

